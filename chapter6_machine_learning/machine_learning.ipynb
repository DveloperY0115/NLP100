{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "131072"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import modules\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set the CSV field limit as large as possible\n",
    "csv.field_size_limit(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset does not exist. Requesting dataset...\n",
      "Downloaded file at: ./test/NewsAggregatorDataset.zip\n",
      "Cleaning up...\n",
      "Dataset is now ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Get the dataset\n",
    "\"\"\"\n",
    "\n",
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip'\n",
    "data_dir = './test'\n",
    "if not os.path.exists(data_dir):\n",
    "    print('Dataset does not exist. Requesting dataset...')\n",
    "    os.mkdir(data_dir)\n",
    "    import wget\n",
    "    # download file\n",
    "    output_dir = data_dir\n",
    "    downloaded_file = wget.download(data_url, out=output_dir)\n",
    "    print('Downloaded file at: {}'.format(downloaded_file))\n",
    "    # unzip and remove the zipped file\n",
    "    import zipfile\n",
    "    zipped_file = zipfile.ZipFile(os.path.join(downloaded_file))\n",
    "    zipped_file.extractall(data_dir)\n",
    "    zipped_file.close()\n",
    "    print('Cleaning up...')\n",
    "    os.remove(os.path.join(downloaded_file))\n",
    "print('Dataset is now ready')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read file and create Pandas DataFrame\n",
    "\"\"\"\n",
    "with open('./data/NewsAggregatorDataset/newsCorpora.csv', \"r\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    columns = ['id', 'title', 'publisher', 'category']\n",
    "\n",
    "    # containers for each field\n",
    "    data_id = []\n",
    "    title = []\n",
    "    publisher = []\n",
    "    category = []\n",
    "\n",
    "    for row in reader:\n",
    "        row = \"\".join(row).split('\\t')\n",
    "        data_id.append(int(row[0]))\n",
    "        title.append(row[1])\n",
    "        publisher.append(row[3])\n",
    "        category.append(row[4])\n",
    "\n",
    "    # create a dataframe and save it to disk\n",
    "    corpus_df = pd.DataFrame(np.array([data_id, title, publisher, category]).T, columns=columns)\n",
    "    csv_name = \"./data/news_aggregator_dataset.csv\"\n",
    "    corpus_df.to_csv(csv_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Reuter articles:  3868\n",
      "# of Huffington Post articles:  2412\n",
      "# of Businessweek articles:  2371\n",
      "# of Contactmusic.com articles:  2288\n",
      "# of Daily Mail articles:  2205\n",
      "           id                                              title  \\\n",
      "0        3150  Lena Dunham Apologizes For Molestation Joke On...   \n",
      "1       63277  Christina Aguilera 'expecting a baby girl' wit...   \n",
      "2      103247  Is Katherine Heigl's Lawsuit The Most Bizarrel...   \n",
      "3       12939  U.S. Navy SEALs take control of North Korean-f...   \n",
      "4       77136  Chrysler Canada says auto sales climb 2 percen...   \n",
      "...       ...                                                ...   \n",
      "13139  296632  Jason Momoa Is Aquaman! But What Do We Know Ab...   \n",
      "13140  219018  Girls Gone Wild's Joe Francis is arrested for ...   \n",
      "13141  123018  HBO No GO: Your Game of Thrones Streaming Woes...   \n",
      "13142   33782  So This Is What Twitter Has To Say About The #...   \n",
      "13143   35625     US forces hand over seized oil tanker to Libya   \n",
      "\n",
      "              publisher category  \n",
      "0       Huffington Post        e  \n",
      "1            Daily Mail        e  \n",
      "2      Contactmusic.com        e  \n",
      "3            Daily Mail        b  \n",
      "4               Reuters        b  \n",
      "...                 ...      ...  \n",
      "13139  Contactmusic.com        e  \n",
      "13140        Daily Mail        e  \n",
      "13141      Businessweek        e  \n",
      "13142   Huffington Post        e  \n",
      "13143           Reuters        b  \n",
      "\n",
      "[13144 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extract articles from selected publishers\n",
    "\"\"\"\n",
    "target_pub = ['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']\n",
    "\n",
    "# Reuters\n",
    "reuter = corpus_df.loc[corpus_df['publisher'] == target_pub[0]]\n",
    "print(\"# of Reuter articles: \", len(reuter))\n",
    "\n",
    "# Huff Post\n",
    "huff_post = corpus_df.loc[corpus_df['publisher'] == target_pub[1]]\n",
    "print(\"# of Huffington Post articles: \", len(huff_post))\n",
    "\n",
    "# Businessweek\n",
    "b_week = corpus_df.loc[corpus_df['publisher'] == target_pub[2]]\n",
    "print(\"# of Businessweek articles: \", len(b_week))\n",
    "\n",
    "# Contactmusic.com\n",
    "con_music = corpus_df.loc[corpus_df['publisher'] == target_pub[3]]\n",
    "print(\"# of Contactmusic.com articles: \", len(con_music))\n",
    "\n",
    "# Daily Mail\n",
    "daily_mail = corpus_df.loc[corpus_df['publisher'] == target_pub[4]]\n",
    "print(\"# of Daily Mail articles: \", len(daily_mail))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split into train, validation, and test dataset and save them\n",
    "\"\"\"\n",
    "\n",
    "selected_articles = pd.concat([reuter, huff_post, b_week, con_music, daily_mail])\n",
    "selected_articles_shuffled = selected_articles.sample(frac=1).reset_index(drop=True)\n",
    "print(selected_articles_shuffled)\n",
    "\n",
    "num_train = int(len(selected_articles_shuffled) * 0.8)\n",
    "num_validation = int((len(selected_articles_shuffled) - num_train) * 0.5)\n",
    "num_test = int(len(selected_articles_shuffled) - num_train - num_validation)\n",
    "\n",
    "# check if there's missing data\n",
    "assert(num_train + num_validation + num_test == len(selected_articles_shuffled))\n",
    "\n",
    "train_data = selected_articles_shuffled.iloc[:num_train]\n",
    "validation_data = selected_articles_shuffled.iloc[num_train:num_train+num_validation]\n",
    "test_data = selected_articles_shuffled.iloc[num_train+num_validation:]\n",
    "\n",
    "train_data.to_csv('./data/train.txt', sep='\\t')\n",
    "validation_data.to_csv('./data/valid.txt', sep='\\t')\n",
    "test_data.to_csv('./data/test.txt', sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "dvelopery0115",
   "language": "python",
   "display_name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}